<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views.">
  <meta name="keywords" content="Hand Motion Forecasting, Egocentric Vision, Hand-Object Interaction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="section">
  <div class="hero-body">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-3 publication-title">Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://bit-mjy.github.io/">Junyi Ma</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://cogito2012.github.io/homepage/">Wentao Bao</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/BIT-XJY">Jingyi Xu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://github.com">Guanzhong Sun</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=M4cXM9kAAAAJ&hl=zh-CN">Yu Zheng</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=j1mUqHEAAAAJ&hl=en">Erhang Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=DvrngV4AAAAJ&hl=zh-CN">Xieyuanli Chen</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=q6AY9XsAAAAJ">Hesheng Wang</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University,</span>
            <span class="author-block"><sup>2</sup>Meta Reality Labs,</span>
            <span class="author-block"><sup>3</sup>China University of Mining and Technology,</span>
            <span class="author-block"><sup>4</sup>National University of Defense Technology</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2504.07375"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/IRMVLab/MMTwin"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/IRMVLab/MMTwin"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>
          </div>
          
          <div class="column has-text-centered">
            <img src="./static/images/unihand_motivation.gif"
                 class="architecture-image"
                 alt="architecture-image image."
                 style="width: 40%; display: block; margin: 0 auto;"/>
            <h2 class="content has-text-justified" style="width: 55%; margin: 0 auto;">
              <strong>Uni-Hand</strong> is a universal framework for hand motion forecasting that supports multi-dimensional, multi-target predictions from multi-modal inputs while enabling multi-task affordances for downstream applications.
            </h2>
          </div>

          <div class="column has-text-centered">
              <video id="teaser" autoplay muted loop playsinline height="50%" 
                     style="width: 50%; box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2), 0 6px 20px 0 rgba(0,0,0,0.19);">
                <source src="./static/videos/compressed_mmtwin_video.mp4" type="video/mp4">
              </video>
              <h2 class="content has-text-justified" style="width: 55%; margin: 0 auto;">
                MMTwin incorporates <strong>RGB images, point clouds, text prompts, and past hand waypoints</strong> to coherently predict future camera egomotion and 3D hand trajectories in egocentric views.
              </h2>
          </div>

          <div class="column has-text-centered">
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                  <p>
                  <strong>We present novel diffusion models (MMTwin) for multimodal 3D hand trajectory prediction.</strong> MMTwin is designed to absorb multimodal information as input encompassing 2D RGB images, 3D point clouds, past hand waypoints, and text prompt. Besides, two latent diffusion models, the egomotion diffusion and the HTP diffusion as twins, are integrated into MMTwin to predict camera egomotion and future hand trajectories concurrently. We propose a novel hybrid Mamba-Transformer module as the denoising model of the HTP diffusion to better fuse multimodal features. The experimental results on three publicly available datasets and our self-recorded data demonstrate that our proposed MMTwin can predict plausible future 3D hand trajectories compared to the state-of-the-art baselines, and generalizes well to unseen environments.</p>
                </div>
              </div>
            </div>
          </div>

          <div class="column has-text-centered">
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <h2 class="title is-3">MMTwin Architecture</h2>
                  <img src="./static/images/mmtwin_arch.png"
                   class="architecture-image"
                   alt="architecture-image image."
                   style="width: 60%; display: block; margin: 0 auto;"/>
                <div class="content has-text-justified">
                  <p>
                  Our proposed MMTwin (a) extracts features from multimodal data, and (b) decouples predictions of future camera egomotion features and 3D hand trajectories by novel twin diffusion models. The vanilla Mamba (VM) is used for denoising in the egomotion diffusion. We further design a new denoising model in HTP diffusion with (c) a hybrid Mamba-Transformer module (HMTM), encompassing the egomotion-aware Mamba (EAM) blocks and (d) the structure-aware Transformer (SAT).
                </div>
              </div>
            </div>
          </div>

          <div class="column has-text-centered">
            <h2 class="title is-3" style="text-align: center;">
              Visualizations with Point Clouds
            </h2>
          </div>
          
            <div class="columns is-centered" style="width: 80%; margin: 0 auto;">
              <div class="column">
              <div class="content" style="max-width: 60%; margin: 0 auto;">
                  <video id="exp1" autoplay muted loop playsinline height="60%">
                    <source src="./static/videos/case1_zoomed.mp4" type="video/mp4">
                  </video>
                </div>
              </div>
        
              <div class="column">
              <div class="content" style="max-width: 60%; margin: 0 auto;">
                  <video id="exp2" autoplay muted loop playsinline height="60%">
                    <source src="./static/videos/case2_zoomed.mp4" type="video/mp4">
                  </video>
                </div>
              </div>
        
              <div class="column">
              <div class="content" style="max-width: 60%; margin: 0 auto;">
                  <video id="exp3" autoplay muted loop playsinline height="60%">
                    <source src="./static/videos/case3_zoomed.mp4" type="video/mp4">
                  </video>
                </div>
              </div>
            </div>
          
            <div class="column has-text-centered">
              <p>
              Green: Past waypoints, Blue: GT future waypoints, Red: MMTwin predictions
            </div>
          

          <div class="column has-text-centered">
              <h2 class="title is-3">Multifinger Predictions: New Path to Human-Robot Policy Transfer?</h2>
              <video id="teaser" autoplay muted loop playsinline height="50%" 
                     style="width: 50%; box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2), 0 6px 20px 0 rgba(0,0,0,0.19);">
                <source src="./static/videos/multi_finger_pred.mp4" type="video/mp4">
              </video>
              <h2 class="content has-text-justified" style="width: 55%; margin: 0 auto;">
                MMTwin can well predict 3D movements of multiple fingers, which exhibits the potential for transfer to robotic skills (we are currently working on).
              </h2>
          </div>


          <div class="column has-text-centered">
              <h2 class="title is-3">CABH Benchmark</h2>
                  <img src="./static/images/CABH_bench.png"
                   class="architecture-image"
                   alt="architecture-image image."
                   style="width: 60%; display: block; margin: 0 auto;"/>
              </video>
              <h2 class="content has-text-justified" style="width: 55%; margin: 0 auto;">
                  We have released our CABH Benchmark to enable fast evaluation of hand trajectory prediction methods and their robot policy transfer potential. Feel free to try it!
              </h2>
          </div>

          <table>
            <thead>
              <tr>
                <th>Task</th>
                <th>Description</th>
                <th>Link (raw)</th>
                <th>Link (preprocessed)</th>
                <th>Link (GLIP feats)</th>
                <th>Link (train/test splits)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>1</td>
                <td>place the cup on the coaster</td>
                <td><a href="https://pan.sjtu.edu.cn/web/share/921173eaddd9f64c609b78bcd0314174" target="_blank">hand_data_red_cup.tar.gz</a></td>
                <td><a href="https://pan.sjtu.edu.cn/web/share/56557c9526a9c2faa37150e8eeb1bca3" target="_blank">hand_data_for_pipeline_mask_redcup.tar.gz</a></td>
                <td><a href="https://pan.sjtu.edu.cn/web/share/1cef4958eea97fe41c889111095c18d5" target="_blank">glip_feats_redcup.tar.gz</a></td>
                <td>
                  <a href="https://pan.sjtu.edu.cn/web/share/f9fa399422307f7b3e32bed2f534c8ff" target="_blank">train_split.txt</a> /
                  <a href="https://pan.sjtu.edu.cn/web/share/548b44a2fbfc0795020d7e51d8b52aa6" target="_blank">test_split.txt</a>
                </td>
              </tr>
              <tr>
                <td>2</td>
                <td>put the apple on the plate</td>
                <td><a href="https://pan.sjtu.edu.cn/web/share/ff0e36b5db1e0192d64d5cbfb5597b5c" target="_blank">hand_data_red_apple.tar.gz</a></td>
                <td><a href="https://pan.sjtu.edu.cn/web/share/064b9fe4e5acaca3408e1293a27eae35" target="_blank">hand_data_for_pipeline_mask_redapple.tar.gz</a></td>
                <td><a href="https://pan.sjtu.edu.cn/web/share/eba393250a4c960a46cb566aaa88c10c" target="_blank">glip_feats_redapple.tar.gz</a></td>
                <td>
                  <a href="https://pan.sjtu.edu.cn/web/share/51236f59c34741084eef0e13378ca6ce" target="_blank">train_split.txt</a> /
                  <a href="https://pan.sjtu.edu.cn/web/share/8eeca488c3212e210b1be65ca25fd128" target="_blank">test_split.txt</a>
                </td>
              </tr>
              <tr>
                <td>3</td>
                <td>place the box on the shelf</td>
                <td><a href="https://pan.sjtu.edu.cn/web/share/898718217ac4b8f0640578e38f04b8d2" target="_blank">hand_data_box.tar.gz</a></td>
                <td><a href="https://pan.sjtu.edu.cn/web/share/56cacb8a5a65dd71dd6cf304bc6e3f19" target="_blank">hand_data_for_pipeline_mask_box.tar.gz</a></td>
                <td><a href="https://pan.sjtu.edu.cn/web/share/13b67a41937e61f8048a2a805290834f" target="_blank">glip_feats_box.tar.gz</a></td>
                <td>
                  <a href="https://pan.sjtu.edu.cn/web/share/338844753b5023ff588f68030226eef9" target="_blank">train_split.txt</a> /
                  <a href="https://pan.sjtu.edu.cn/web/share/7bcd035fbbb4cff348ab9cbe5d59114f" target="_blank">test_split.txt</a>
                </td>
              </tr>
            </tbody>
          </table>
          
          <style>
            table {
              width: 100%;
              border-collapse: collapse;
              margin: 20px 0;
              font-family: Arial, sans-serif;
            }
            th, td {
              border: 1px solid #ddd;
              padding: 8px 12px;
              text-align: left;
            }
            th {
              background-color: #f2f2f2;
              font-weight: bold;
            }
            tr:nth-child(even) {
              background-color: #f9f9f9;
            }
            a {
              color: #0366d6;
              text-decoration: none;
            }
            a:hover {
              text-decoration: underline;
            }
          </style>          

          <div class="column has-text-centered">
              <h2 class="content has-text-justified" style="width: 55%; margin: 0 auto;">
              <p>Please refer to our <a href="https://github.com/IRMVLab/MMTwin">repo</a> for instructions on how to use this benchmark.</p>
              </h2>
          </div>



          
          
        </div>
    </div>
  </div>
</section>












  
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{ma2025mmtwin,
      title={Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction}, 
      author={Junyi Ma and Wentao Bao and Jingyi Xu and Guanzhong Sun and Xieyuanli Chen and Hesheng Wang},
      year={2025},
      eprint={2504.07375},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2504.07375}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2504.07375">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/BIT-MJY" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>



</body>
</html>
